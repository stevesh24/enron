{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nexternal classifier.py \\n\\n0. read in data (df_test2, df_test2_subject_cleaned)\\n1. classifier predicting is_external (binary) from body (using Random Forest)\\n        input: df_test2.csv, rows = 6467, 1.25% of original enron dataset\\n2. classifier predicting is_external (binary) from subject (using Random Forest)\\n        input: df_test2_subject_cleaned.csv, rows = 6235, 1.25% of original enron dataset\\n3. hyperparameter tuning of classifier predicting is_external (binary) from body (using Random Forest, Grid Search)       \\n'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "external classifier.ipynb\n",
    "\n",
    "0. read in data (df_test2, df_test2_subject_cleaned)\n",
    "1. classifier predicting is_external (binary) from body (using Random Forest)\n",
    "        input: df_test2.csv, rows = 6467, 1.25% of original enron dataset\n",
    "2. classifier predicting is_external (binary) from subject (using Random Forest)\n",
    "        input: df_test2_subject_cleaned.csv, rows = 6235, 1.25% of original enron dataset\n",
    "3. hyperparameter tuning of classifier predicting is_external (binary) from body (using Random Forest, Grid Search)       \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import math\n",
    "import sys\n",
    "import copy\n",
    "import sklearn\n",
    "import nltk\n",
    "import re\n",
    "import email\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import TruncatedSVD \n",
    "from sklearn.preprocessing import normalize \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# 0. read in data (df_test2, df_test2_subject_cleaned)\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test2.csv, rows = 6467, 1.25% of original enron dataset\n",
    "# output: df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6467, 13)\n",
      "Index(['index_orig', 'file', 'message', 'email_to', 'email_from', 'cc', 'bcc',\n",
      "       'subject', 'x_from', 'x_to', 'date_sent', 'body', 'is_external'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('df_test2.csv')\n",
    "print(df.shape)\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6467, 14)\n",
      "Index(['index', 'index_orig', 'file', 'message', 'email_to', 'email_from',\n",
      "       'cc', 'bcc', 'subject', 'x_from', 'x_to', 'date_sent', 'body',\n",
      "       'is_external'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# filter out null body, is_external (sanity check)\n",
    "\n",
    "df.dropna(subset = ['body', 'is_external'], inplace = True)\n",
    "df.reset_index(inplace = True)\n",
    "print(df.shape)\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert is_external to int\n",
    "\n",
    "df['is_external'] = df['is_external'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    6467.000000\n",
       "mean        0.329519\n",
       "std         0.470075\n",
       "min         0.000000\n",
       "25%         0.000000\n",
       "50%         0.000000\n",
       "75%         1.000000\n",
       "max         1.000000\n",
       "Name: is_external, dtype: float64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['is_external'].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test2_subject_cleaned.csv, rows = 6235, 1.25% of original enron dataset\n",
    "# output: df_sub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6241, 13)\n",
      "Index(['index_orig', 'file', 'message', 'email_to', 'email_from', 'cc', 'bcc',\n",
      "       'subject', 'x_from', 'x_to', 'date_sent', 'body', 'is_external'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df_sub = pd.read_csv('df_test2_clean_subject.csv')\n",
    "print(df_sub.shape)\n",
    "print(df_sub.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6241, 14)\n",
      "Index(['index', 'index_orig', 'file', 'message', 'email_to', 'email_from',\n",
      "       'cc', 'bcc', 'subject', 'x_from', 'x_to', 'date_sent', 'body',\n",
      "       'is_external'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# filter out null subject, is_external (sanity check)\n",
    "\n",
    "df_sub.dropna(inplace = True, subset = ['subject', 'is_external'])\n",
    "df_sub.reset_index(inplace = True)\n",
    "print(df_sub.shape)\n",
    "print(df_sub.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert is_external to int\n",
    "\n",
    "df_sub['is_external'] = df_sub['is_external'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    6241.000000\n",
       "mean        0.328954\n",
       "std         0.469871\n",
       "min         0.000000\n",
       "25%         0.000000\n",
       "50%         0.000000\n",
       "75%         1.000000\n",
       "max         1.000000\n",
       "Name: is_external, dtype: float64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub['is_external'].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# 1. classifier predicting is_external (binary) from body (using Random Forest)\n",
    "#         input: df_test2.csv, rows = 6467, 1.25% of original enron dataset\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign body and is_external to X, y\n",
    "\n",
    "X, y = df['body'], df['is_external']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean and lemmatize\n",
    "# from https://stackabuse.com/text-classification-with-python-and-scikit-learn/\n",
    "\n",
    "documents = []\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "for sen in range(0, len(X)):\n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', str(X[sen]))\n",
    "    \n",
    "    # remove all single characters. I'm keeping for now\n",
    "    #document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "    \n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    \n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "    \n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    \n",
    "    # Lemmatization\n",
    "    document = document.split()\n",
    "\n",
    "    document = [stemmer.lemmatize(the_word) for the_word in document]\n",
    "    document = ' '.join(document)\n",
    "    \n",
    "    documents.append(document)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=50, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "X = vectorizer.fit_transform(documents).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply tfidf\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidfconverter = TfidfTransformer()\n",
    "X = tfidfconverter.fit_transform(X).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train and test (80/20)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit training data using Random Forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifier = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "classifier.fit(X_train, y_train) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3427   43]\n",
      " [ 233 1470]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.96      3470\n",
      "           1       0.97      0.86      0.91      1703\n",
      "\n",
      "    accuracy                           0.95      5173\n",
      "   macro avg       0.95      0.93      0.94      5173\n",
      "weighted avg       0.95      0.95      0.95      5173\n",
      "\n",
      "0.9466460467813648\n",
      "0.9253953440360375\n"
     ]
    }
   ],
   "source": [
    "# metrics for training set\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score\n",
    "\n",
    "y_pred = classifier.predict(X_train)\n",
    "print(confusion_matrix(y_train,y_pred))\n",
    "print(classification_report(y_train,y_pred))\n",
    "print(accuracy_score(y_train, y_pred))\n",
    "print(roc_auc_score(y_train, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[780  86]\n",
      " [200 228]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.90      0.85       866\n",
      "           1       0.73      0.53      0.61       428\n",
      "\n",
      "    accuracy                           0.78      1294\n",
      "   macro avg       0.76      0.72      0.73      1294\n",
      "weighted avg       0.77      0.78      0.77      1294\n",
      "\n",
      "0.7789799072642968\n",
      "0.7167015605102415\n"
     ]
    }
   ],
   "source": [
    "# metrics for test set\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(roc_auc_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'auto',\n",
       " 'max_leaf_nodes': None,\n",
       " 'max_samples': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_impurity_split': None,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': 0,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at default hyperparameters\n",
    "\n",
    "classifier.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# 2. classifier predicting is_external (binary) from subject (using Random Forest)\n",
    "#        input: df_test2_subject_cleaned.csv, rows = 6235, 1.25% of original enron dataset#\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign body and is_external to X, y\n",
    "\n",
    "X, y = df_sub['subject'], df_sub['is_external']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean and lemmatize\n",
    "# from https://stackabuse.com/text-classification-with-python-and-scikit-learn/\n",
    "\n",
    "documents = []\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "for sen in range(0, len(X)):\n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', str(X[sen]))\n",
    "    \n",
    "    # remove all single characters. I'm keeping for now\n",
    "    #document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "    \n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    \n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "    \n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    \n",
    "    # Lemmatization\n",
    "    document = document.split()\n",
    "\n",
    "    document = [stemmer.lemmatize(the_word) for the_word in document]\n",
    "    document = ' '.join(document)\n",
    "    \n",
    "    documents.append(document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
       "                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vectorize\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=500, min_df=2, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "X = vectorizer.fit_transform(documents).toarray()\n",
    "\n",
    "# apply tfidf\n",
    "\n",
    "tfidfconverter = TfidfTransformer()\n",
    "X = tfidfconverter.fit_transform(X).toarray()\n",
    "\n",
    "# split train and test (80/20)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# fit training data using Random Forest\n",
    "\n",
    "classifier = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "classifier.fit(X_train, y_train) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3254   96]\n",
      " [ 571 1071]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.97      0.91      3350\n",
      "           1       0.92      0.65      0.76      1642\n",
      "\n",
      "    accuracy                           0.87      4992\n",
      "   macro avg       0.88      0.81      0.83      4992\n",
      "weighted avg       0.87      0.87      0.86      4992\n",
      "\n",
      "0.866386217948718\n",
      "0.8117983165778901\n"
     ]
    }
   ],
   "source": [
    "# metrics for training set\n",
    "\n",
    "y_pred = classifier.predict(X_train)\n",
    "print(confusion_matrix(y_train,y_pred))\n",
    "print(classification_report(y_train,y_pred))\n",
    "print(accuracy_score(y_train, y_pred))\n",
    "print(roc_auc_score(y_train, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[728 110]\n",
      " [291 120]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.87      0.78       838\n",
      "           1       0.52      0.29      0.37       411\n",
      "\n",
      "    accuracy                           0.68      1249\n",
      "   macro avg       0.62      0.58      0.58      1249\n",
      "weighted avg       0.65      0.68      0.65      1249\n",
      "\n",
      "0.6789431545236189\n",
      "0.5803529432259639\n"
     ]
    }
   ],
   "source": [
    "# metrics for test set\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(roc_auc_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'auto',\n",
       " 'max_leaf_nodes': None,\n",
       " 'max_samples': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_impurity_split': None,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 1000,\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': 0,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at default hyperparameters\n",
    "\n",
    "classifier.get_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# 3. hyperparameter tuning of classifier predicting is_external (binary) from body (using Random Forest, Grid Search)       \n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign body and is_external to X, y\n",
    "\n",
    "X, y = df['body'], df['is_external']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean and lemmatize\n",
    "# from https://stackabuse.com/text-classification-with-python-and-scikit-learn/\n",
    "\n",
    "documents = []\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "for sen in range(0, len(X)):\n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', str(X[sen]))\n",
    "    \n",
    "    # remove all single characters. I'm keeping for now\n",
    "    #document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "    \n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    \n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "    \n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    \n",
    "    # Lemmatization\n",
    "    document = document.split()\n",
    "\n",
    "    document = [stemmer.lemmatize(the_word) for the_word in document]\n",
    "    document = ' '.join(document)\n",
    "    \n",
    "    documents.append(document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize\n",
    "vectorizer = CountVectorizer(max_features=50, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "X = vectorizer.fit_transform(documents).toarray()\n",
    "\n",
    "# apply tfidf\n",
    "tfidfconverter = TfidfTransformer()\n",
    "X = tfidfconverter.fit_transform(X).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nn_estimators = number of trees in the foreset\\nmax_features = max number of features considered for splitting a node\\nmax_depth = max number of levels in each decision tree\\nmin_samples_split = min number of data points placed in a node before the node is split\\nmin_samples_leaf = min number of data points allowed in a leaf node\\nbootstrap = method for sampling data points (with or without replacement)\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyperparameters to tune\n",
    "\n",
    "''' \n",
    "n_estimators = number of trees in the foreset\n",
    "max_features = max number of features considered for splitting a node\n",
    "max_depth = max number of levels in each decision tree\n",
    "min_samples_split = min number of data points placed in a node before the node is split\n",
    "min_samples_leaf = min number of data points allowed in a leaf node\n",
    "bootstrap = method for sampling data points (with or without replacement)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\n'bootstrap': True,\\n 'ccp_alpha': 0.0,\\n 'class_weight': None,\\n 'criterion': 'gini',\\n 'max_depth': 50,\\n 'max_features': 'auto',\\n 'max_leaf_nodes': None,\\n 'max_samples': None,\\n 'min_impurity_decrease': 0.0,\\n 'min_impurity_split': None,\\n 'min_samples_leaf': 2,\\n 'min_samples_split': 5,\\n 'min_weight_fraction_leaf': 0.0,\\n 'n_estimators': 2000,\\n 'n_jobs': None,\\n 'oob_score': False,\\n 'random_state': 0,\\n 'verbose': 0,\\n 'warm_start': False}\\n\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# default hyperparameters from first run above\n",
    "\n",
    "''' \n",
    "'bootstrap': True,\n",
    " 'ccp_alpha': 0.0,\n",
    " 'class_weight': None,\n",
    " 'criterion': 'gini',\n",
    " 'max_depth': 50,\n",
    " 'max_features': 'auto',\n",
    " 'max_leaf_nodes': None,\n",
    " 'max_samples': None,\n",
    " 'min_impurity_decrease': 0.0,\n",
    " 'min_impurity_split': None,\n",
    " 'min_samples_leaf': 2,\n",
    " 'min_samples_split': 5,\n",
    " 'min_weight_fraction_leaf': 0.0,\n",
    " 'n_estimators': 2000,\n",
    " 'n_jobs': None,\n",
    " 'oob_score': False,\n",
    " 'random_state': 0,\n",
    " 'verbose': 0,\n",
    " 'warm_start': False}\n",
    "'''\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose small hyperparameter range of target hyperparameters and around the default values\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'bootstrap': [False,True],\n",
    "    'max_depth': [25,50,75],\n",
    "    'max_features': ['auto'],\n",
    "    'min_samples_leaf': [1,2,3],\n",
    "    'min_samples_split': [4,5,6],\n",
    "    'n_estimators': [1500, 2000, 2500]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create classifier classes\n",
    "\n",
    "classifier = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "classifier_grid = GridSearchCV(estimator = classifier, param_grid = param_grid, cv = 3, n_jobs = -1, verbose = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 162 candidates, totalling 486 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed: 15.7min\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed: 110.7min\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed: 280.3min\n",
      "[Parallel(n_jobs=-1)]: Done 486 out of 486 | elapsed: 379.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score=nan,\n",
       "             estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                                              class_weight=None,\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features='auto',\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              max_samples=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              n_estimators=100, n_jobs=None,\n",
       "                                              oob_score=False, random_state=0,\n",
       "                                              verbose=0, warm_start=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'bootstrap': [False, True], 'max_depth': [25, 50, 75],\n",
       "                         'max_features': ['auto'],\n",
       "                         'min_samples_leaf': [1, 2, 3],\n",
       "                         'min_samples_split': [4, 5, 6],\n",
       "                         'n_estimators': [1500, 2000, 2500]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=2)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit (3-fold cross validation)\n",
    "\n",
    "classifier_grid.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=50, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=2, min_samples_split=6,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=1500,\n",
      "                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
      "                       warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "# find best model from GridSearch\n",
    "\n",
    "best_grid = classifier_grid.best_estimator_\n",
    "print(best_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3310   60]\n",
      " [1256  361]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.98      0.83      3370\n",
      "           1       0.86      0.22      0.35      1617\n",
      "\n",
      "    accuracy                           0.74      4987\n",
      "   macro avg       0.79      0.60      0.59      4987\n",
      "weighted avg       0.77      0.74      0.68      4987\n",
      "\n",
      "0.7361138961299378\n",
      "0.6027243916179907\n"
     ]
    }
   ],
   "source": [
    "# metrics for training set\n",
    "\n",
    "y_pred = best_grid.predict(X_train)\n",
    "print(confusion_matrix(y_train,y_pred))\n",
    "print(classification_report(y_train,y_pred))\n",
    "print(accuracy_score(y_train, y_pred))\n",
    "print(roc_auc_score(y_train, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[796  28]\n",
      " [365  58]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.97      0.80       824\n",
      "           1       0.67      0.14      0.23       423\n",
      "\n",
      "    accuracy                           0.68      1247\n",
      "   macro avg       0.68      0.55      0.51      1247\n",
      "weighted avg       0.68      0.68      0.61      1247\n",
      "\n",
      "0.6848436246992783\n",
      "0.5515676283596135\n"
     ]
    }
   ],
   "source": [
    "# metrics for test set\n",
    "\n",
    "y_pred = best_grid.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(roc_auc_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
